{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "# Custom Training Basics\n",
    "\n",
    "In this ungraded lab you'll gain a basic understanding of building custom training loops. \n",
    "- It takes you through the underlying logic of fitting any model to a set of inputs and outputs. \n",
    "- You will be training your model on the linear equation for a straight line, wx + b. \n",
    "- You will implement basic linear regression from scratch using gradient tape.\n",
    "- You will try to minimize the loss incurred by the model using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiolgWMPgpwI"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7O6eEGF5DcN"
   },
   "source": [
    "## Define Model\n",
    "\n",
    "You define your model as a class. \n",
    "- `x` is your input tensor. \n",
    "- The model should output values of **wx+b**. \n",
    "- You'll start off by initializing w and b to random values. \n",
    "- During the training process, values of w and b get updated in accordance with linear regression so as to minimize the loss incurred by the model. \n",
    "- Once you arrive at optimal values for w and b, the model would have been trained to correctly predict the values of wx+b.\n",
    "\n",
    "Hence, \n",
    "- **w** and **b** are trainable weights of the model. \n",
    "- **x** is the input\n",
    "- **y** = wx + b is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WRu7Pze7wk8"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "  def __init__(self):\n",
    "    # Initialize the weights to `2.0` and the bias to `1.0`\n",
    "    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n",
    "    self.w = tf.Variable(2.0)\n",
    "    self.b = tf.Variable(1.0)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.w * x + self.b\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa6j_yXa-j79"
   },
   "source": [
    "### Define a loss function\n",
    "\n",
    "A loss function measures how well the output of a model for a given input matches the target output. \n",
    "- The goal is to minimize this difference during training. \n",
    "- Let's use the standard L2 loss, also known as the least square errors\n",
    "$$Loss = \\sum_{i} \\left (y_{pred}^i - y_{target}^i \\right )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0ysUFGY924U"
   },
   "outputs": [],
   "source": [
    "def loss(predicted_y, target_y):\n",
    "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "### Obtain training data\n",
    "\n",
    "First, synthesize the training data using the \"true\" w and \"true\" b. \n",
    "\n",
    "$$y = w_{true} \\times x + b_{true} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "TRUE_w = 3.0\n",
    "TRUE_b = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "xs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "ys = (TRUE_w * xs) + TRUE_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-50nq-wPBsAW"
   },
   "source": [
    "Before training the model, visualize the loss value by plotting the model's predictions in red crosses and the training data in blue dots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eb83LtrB4nt"
   },
   "outputs": [],
   "source": [
    "def plot_data(inputs, outputs, predicted_outputs):\n",
    "  real = plt.scatter(inputs, outputs, c='b', marker='.')\n",
    "  predicted = plt.scatter(inputs, predicted_outputs, c='r', marker='+')\n",
    "  plt.legend((real,predicted), ('Real Data', 'Predicted Data'))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XL25a_aEOuim"
   },
   "outputs": [],
   "source": [
    "plot_data(xs, ys, model(xs))\n",
    "print('Current loss: %1.6f' % loss(model(xs), ys).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSDP-yeq_4jE"
   },
   "source": [
    "### Define a training loop\n",
    "\n",
    "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) \n",
    "- Gradient descent updates the trainable weights **w** and **b** to reduce the loss. \n",
    "\n",
    "\n",
    "There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`â€”our recommended implementation. In the spirit of building from first principles, here you will implement the basic math yourself.\n",
    "- You'll use `tf.GradientTape` for automatic differentiation\n",
    "- Use `tf.assign_sub` for decrementing a value.  Note that assign_sub combines `tf.assign` and `tf.sub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBIACgdnA55X"
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "  model.w.assign_sub(learning_rate * dw)\n",
    "  model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "  return current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Finally, you can iteratively run through the training data and see how `w` and `b` evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdfkR223D9dW"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Collect the history of W-values and b-values to plot later\n",
    "list_w, list_b = [], []\n",
    "epochs = range(15)\n",
    "losses = []\n",
    "for epoch in epochs:\n",
    "  list_w.append(model.w.numpy())\n",
    "  list_b.append(model.b.numpy())\n",
    "  current_loss = train(model, xs, ys, learning_rate=0.1)\n",
    "  losses.append(current_loss)\n",
    "  print('Epoch %2d: w=%1.2f b=%1.2f, loss=%2.5f' %\n",
    "        (epoch, list_w[-1], list_b[-1], current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EI_1PwOBR6TW"
   },
   "source": [
    "In addition to the values for losses, you also plot the progression of trainable variables over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8gJThOCNXAp"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, list_w, 'r',\n",
    "       epochs, list_b, 'b')\n",
    "plt.plot([TRUE_w] * len(epochs), 'r--',\n",
    "      [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['w', 'b', 'True w', 'True b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsTbG9J2MM9W"
   },
   "source": [
    "## Plots for Evaluation\n",
    "Now you can plot the actual outputs in red and the model's predictions in blue on a set of random test examples.\n",
    "\n",
    "You can see that the model is able to make predictions on the test set fairly accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRHpHCJ3273d"
   },
   "outputs": [],
   "source": [
    "test_inputs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "test_outputs = test_inputs * TRUE_w + TRUE_b\n",
    "\n",
    "predicted_test_outputs = model(test_inputs)\n",
    "plot_data(test_inputs, test_outputs, predicted_test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY-j2FJYSfis"
   },
   "source": [
    "Visualize the cost function against the values of each of the trainable weights the model approximated to over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY-gQWFfOIu-"
   },
   "outputs": [],
   "source": [
    "def plot_loss_for_weights(weights_list, losses):\n",
    "  for idx, weights in enumerate(weights_list):\n",
    "    plt.subplot(120 + idx + 1)\n",
    "    plt.plot(weights['values'], losses, 'r')\n",
    "    plt.plot(weights['values'], losses, 'bo')\n",
    "    plt.xlabel(weights['name'])\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    \n",
    "weights_list = [{ 'name' : \"w\",\n",
    "                  'values' : list_w\n",
    "                },\n",
    "                {\n",
    "                  'name' : \"b\",\n",
    "                  'values' : list_b\n",
    "                }]\n",
    "\n",
    "plot_loss_for_weights(weights_list, losses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Training Basics.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
